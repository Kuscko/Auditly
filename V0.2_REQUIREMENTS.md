# RapidRMF v0.2 Requirements & Roadmap

**Status**: In Progress  
**Target Phases**: 10 weeks  
**Priority**: Database foundation first, then multi-cloud, then packaging

**Foundation Complete (Weeks 1-4)**: ✅ Database layer, scheduled validation, and minimal REST API implemented

---

## Phased Approach

### Weeks 1–4: Foundation (Database & Scheduled Validation)
**Recommended to start here** - Everything else depends on this layer.

Current status (as of Jan 2026):
- Database layer implemented and validated end-to-end with PostgreSQL
- Integration tests added and running (tests/integration)
- Next focus: Scheduled validation framework and job state persistence

#### Database Layer
- [x] PostgreSQL backend with SQLAlchemy 2.0 models for:
  - Evidence
  - Findings
  - Systems
  - Controls
  - Validation Results
- [x] Alembic migration structure (production-ready; env var + CLI overrides)
- [x] Database backend that mirrors current file-based storage (repository layer)
- [ ] Backward compatibility / migration tool to transition from file-based to PostgreSQL
  - Note: Initial utility exists (`rapidrmf/db/file_migration.py`) – expand and document

#### Scheduled Validation
- [x] Nightly/cron job scheduling for validation runs
  - Implementation: APScheduler-based scheduler with daily window; configurable via `config.yaml`
  - CLI: `rapidrmf scheduler start`, `once`, `runs`
- [x] Job state persistence in database
  - Implementation: `job_runs` table to persist status, start/end times, metrics
  - Records: job type, environment, status (success/fail), errors, metrics
  - Full validation flow wired to DB persistence

#### Minimal REST API
- [x] `/collect` endpoint - trigger collection from cloud providers
- [x] `/validate` endpoint - run validation against collected evidence
- [x] `/report` endpoint - generate compliance reports from findings
- **Implementation**: FastAPI-based REST API in `rapidrmf/api/`
  - Reuses existing CLI/collector/validator logic (DRY principle)
  - Swagger UI docs at `/docs`
  - Supports all providers: terraform, github, gitlab, argo, azure
  - See `rapidrmf/api/README.md` for usage

---

### Weeks 5–8: Multi-Cloud Collectors
**Depends on**: Database foundation (Weeks 1–4)

#### AWS Collector
- [ ] EC2 resources
- [ ] RDS databases
- [ ] S3 buckets
- [ ] IAM policies and roles
- [ ] CloudTrail logs
- [ ] AWS Config

#### GCP Collector
- [ ] Compute instances
- [ ] Cloud SQL databases
- [ ] Cloud Storage buckets
- [ ] IAM bindings
- [ ] Cloud Audit Logs

#### Evidence & Mappings
- [ ] Evidence manifests for AWS and GCP
- [ ] Control mappings for both cloud providers

---

### Weeks 9–10: Frameworks & Packaging
**Depends on**: Collectors (Weeks 5–8)

#### Control Catalogs
- [ ] HIPAA catalog integration
- [ ] PCI DSS catalog integration
- [ ] ISO 27001 catalog integration

#### Reporting & Export
- [ ] POA&M (Plan of Action and Milestones) export from findings
- [ ] Enhanced compliance reporting

#### Deployment
- [ ] Helm charts for Kubernetes deployment
- [ ] Docker containerization support

---

## Implementation Questions Answered

**Q: Should I start with database models, REST API, or collectors first?**  
**A: Start with database models and Alembic migrations.** This is the foundation layer—all collectors, validation, and API endpoints will depend on persisting to PostgreSQL. Without this foundation, later phases cannot proceed.

**Implementation Order**:
1. SQLAlchemy 2.0 models (evidence, findings, systems, controls, validation results) – Completed
2. Alembic migration framework – Completed
3. Database repository layer – Completed
4. Backward compatibility tool (file → PostgreSQL migration) – Pending
5. Scheduled job system – Next
6. REST API endpoints – Pending
7. Then proceed to multi-cloud collectors – Pending

---

## Key Implementation Notes

- **Database Design**: Mirror current file-based evidence storage structure as initial schema
- **Backward Compatibility**: Plan migration strategy for existing evidence files
- **Scheduling**: Use APScheduler for cron-like job scheduling (daily/nightly windows)
- **API Framework**: Recommend FastAPI for minimal REST layer
- **ORM**: SQLAlchemy 2.0 with async support for better concurrency
- **Integration Tests**: PostgreSQL E2E added under `tests/integration` with Docker Compose

---

## Files to Reference

- [alembic.ini](alembic.ini) - Migration configuration
- [rapidrmf/db/models.py](rapidrmf/db/models.py) - Existing model definitions
- [rapidrmf/db/migrate.py](rapidrmf/db/migrate.py) - Migration utilities
- [rapidrmf/db/repository.py](rapidrmf/db/repository.py) - Data access layer
- [requirements.txt](requirements.txt) - Current dependencies
- [tests/integration/README.md](tests/integration/README.md) - Database E2E test setup and coverage

---

## Next Agent Checklist

- [x] Review this document and existing database setup
- [x] Confirm SQLAlchemy 2.0 and Alembic versions in requirements.txt
- [x] Expand evidence/findings/systems/controls models as needed
- [x] Create initial Alembic migration for schema
- [ ] Build file-to-database migration tool
- [x] Implement scheduled validation job framework
- [x] Create minimal REST API with three endpoints
- [x] Test end-to-end collection → validation → storage flow (PostgreSQL E2E)

### ✅ Foundation Complete (Weeks 1-4)

All core foundation items are now implemented:
1. ✅ Database layer with PostgreSQL + SQLAlchemy 2.0 + Alembic
2. ✅ Scheduled validation with APScheduler + job runs tracking
3. ✅ Minimal REST API with `/collect`, `/validate`, `/report` endpoints

**Next Phase**: Multi-Cloud Collectors (Weeks 5-8)
- AWS Collector (EC2, RDS, S3, IAM, CloudTrail, Config)
- GCP Collector (Compute, Cloud SQL, Storage, IAM, Audit Logs)
- Evidence manifests and control mappings for both providers

